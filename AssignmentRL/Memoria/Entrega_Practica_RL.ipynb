{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizaje por Refuerzo\n",
    "\n",
    "El aprendizaje por refuerzo (RL) se basa en la idea de que un agente aprende a tomar decisiones interactuando con un entorno. De esta forma, el agente aprende a maximizar una recompensa basada en sus acciones, lo que le permite aprender a realizar tareas complejas a través de la exploración del entorno y la adaptación a los cambios en él.\n",
    "\n",
    "En el presente trabajo, se utilizará RL para entrenar a un agente que controle el movimiento del robot `Acrobot`, utilizando el paquete gym de OpenAI. \n",
    "\n",
    "El entorno Acrobot consiste en un robot de dos brazos que puede girar alrededor de su base. El robot se encuentra en un estado inicial colgando hacia abajo y debe alcanzar una posición objetivo que se define como el momento en que el extremo superior del segundo brazo del robot alcanza una altura específica. En la siguiente figura se representa un movimiento posible de este robot desde su posición inicial sin alcanzar la recompensa:\n",
    "\n",
    "![Movimiento acrobot](Images/acrobot.gif)\n",
    "\n",
    "Veamos en profundidad el espacio de acciones y el entorno del agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('Acrobot-v1')\n",
    "env.seed(42)\n",
    "\n",
    "env.action_space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver en el resultado de la celda anterior el espacio de acciones del agente es dicreto y contiene únicamente 3 acciones. Siguiendo la misma nomenclatura que en la documentación, las acciones posibles son las siguientes:\n",
    "* Acción 0: Aplicar torque negativo a la primera articulación (-1)\n",
    "* Acción 1: No aplicar torque (0)\n",
    "* Acción 2: Aplicar torque positivo a la primera articulación (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.9996832 , -0.02516867,  0.9999951 , -0.00313228,  0.08415417,\n",
       "       -0.0036109 ], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por otro lado, el entorno viene dado por un vector de seis dimensiones representando las posiciones y velocidades los eslabones del acrobot. Denominando $\\theta_1$ al ángulo de la primera unión y $\\theta_2$ al de la segunda, los parámetros del entorno son los siguientes\n",
    "\n",
    "* $\\cos{(\\theta_1)}$\n",
    "\n",
    "* $\\sin{(\\theta_1)}$\n",
    "\n",
    "* $\\cos{(\\theta_2)}$\n",
    "\n",
    "* $\\sin{(\\theta_2)}$\n",
    "\n",
    "* $\\dot{\\theta_1}$: Velocidad angular de $\\theta_1$ \n",
    "\n",
    "* $\\dot{\\theta_1}$: Velocidad angular de $\\theta_2$\n",
    "\n",
    "Las funciones trigonométricas toman valores entre $[-1,1]$, trivialmente, y las velocidades angulares varían entre $[-4\\pi,4\\pi]$ y $[-9\\pi, 9\\pi]$ para $\\dot{\\theta_1}$ y $\\dot{\\theta_2}$, respectivamente"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivos establecidos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo del trabajo es entrenar un agente que sea capaz de controlar con éxito el movimiento del robot Acrobot y alcanzar la posición objetivo en el menor número de episodios posible. Además, se busca analizar y comparar la eficacia de los métodos seleccionados para resolver el problema y así identificar el método más eficiente para la tarea en cuestión."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métodos seleccionados"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La selección de los métodos de aprendizaje por refuerzo utilizados en este trabajo se basa en un análisis detallado del entorno Acrobot y en la adaptabilidad de los métodos de RL a este entorno en particular. \n",
    "\n",
    "En primer lugar, cabe destacar que debido al espacio de observaciones continuo no se puede usar `Q-Learning` y, por extensión, tampoco `Double Q-Learning`. Por otro lado, como el espacio de acciones es discreto también es necesario descartar `DDPG` y `TD3` ya que ambos trabajan sobre un espacio continuo de acciones.\n",
    "\n",
    "Por lo tanto, se han elegido los métodos más adecuados para el problema que se desea resolver.\n",
    "\n",
    "- `DQN` es una buena opción ya que es una generalización de Q-learning que utiliza una red neuronal profunda para aproximar el valor de Q. Se ha demostrado que funciona bien en espacios de observaciones continuos como Acrobot, lo que lo convierte en una opción viable para nuestro problema.\n",
    "\n",
    "- `Double DQN` es una mejora de DQN que ayuda a reducir la sobreestimación del valor de Q, lo que lo hace aún más efectivo en entornos complejos. Al igual que DQN, se ha demostrado que funciona bien en entornos continuos.\n",
    "\n",
    "- `Reinforce` es un método de gradiente de políticas que funciona bien en entornos con acciones continuas y puede ser útil para explorar diferentes estrategias de acción en Acrobot."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entorno de trabajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"Acrobot-v1\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
