{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CARTPOLE GAME SETTINGS\n",
    "OBSERVATION_SPACE_DIMS = 4\n",
    "ACTION_SPACE = [0,1]\n",
    "\n",
    "# AGENT/NETWORK HYPERPARAMETERS\n",
    "EPSILON_INITIAL = 0.5 # exploration rate\n",
    "EPSILON_DECAY = 0.99\n",
    "EPSILON_MIN = 0.01\n",
    "ALPHA = 0.001 # learning rate\n",
    "GAMMA = 0.99 # discount factor\n",
    "TAU = 0.1 # target network soft update hyperparameter\n",
    "EXPERIENCE_REPLAY_BATCH_SIZE = 32\n",
    "AGENT_MEMORY_LIMIT = 2000\n",
    "MIN_MEMORY_FOR_EXPERIENCE_REPLAY = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dqn():\n",
    "    # not actually that deep\n",
    "    nn = Sequential()\n",
    "    nn.add(Dense(64, input_dim=OBSERVATION_SPACE_DIMS, activation='relu'))\n",
    "    nn.add(Dense(64, activation='relu'))\n",
    "    nn.add(Dense(len(ACTION_SPACE), activation='linear'))\n",
    "    nn.compile(loss='mse', optimizer=Adam(lr=ALPHA))\n",
    "    return nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleDQNAgent(object):\n",
    "\n",
    "       \n",
    "    def __init__(self):\n",
    "        self.memory = []\n",
    "        self.online_network = create_dqn()\n",
    "        self.target_network = create_dqn()\n",
    "        self.epsilon = EPSILON_INITIAL\n",
    "        self.has_talked = False\n",
    "    \n",
    "    \n",
    "    def act(self, state):\n",
    "        if self.epsilon > np.random.rand():\n",
    "            # explore\n",
    "            return np.random.choice(ACTION_SPACE)\n",
    "        else:\n",
    "            # exploit\n",
    "            state = self._reshape_state_for_net(state)\n",
    "            q_values = self.online_network.predict(state)[0]\n",
    "            return np.argmax(q_values)\n",
    "\n",
    "\n",
    "    def experience_replay(self):\n",
    "\n",
    "        minibatch = random.sample(self.memory, EXPERIENCE_REPLAY_BATCH_SIZE)\n",
    "        minibatch_new_q_values = []\n",
    "\n",
    "        for experience in minibatch:\n",
    "            state, action, reward, next_state, done = experience\n",
    "            state = self._reshape_state_for_net(state)\n",
    "            experience_new_q_values = self.online_network.predict(state)[0]\n",
    "            if done:\n",
    "                q_update = reward\n",
    "            else:\n",
    "                next_state = self._reshape_state_for_net(next_state)\n",
    "                # using online network to SELECT action\n",
    "                online_net_selected_action = np.argmax(self.online_network.predict(next_state))\n",
    "                # using target network to EVALUATE action\n",
    "                target_net_evaluated_q_value = self.target_network.predict(next_state)[0][online_net_selected_action]\n",
    "                q_update = reward + GAMMA * target_net_evaluated_q_value\n",
    "            experience_new_q_values[action] = q_update\n",
    "            minibatch_new_q_values.append(experience_new_q_values)\n",
    "        minibatch_states = np.array([e[0] for e in minibatch])\n",
    "        minibatch_new_q_values = np.array(minibatch_new_q_values)\n",
    "        self.online_network.fit(minibatch_states, minibatch_new_q_values, verbose=False, epochs=1)\n",
    "        \n",
    "        \n",
    "    def update_target_network(self):\n",
    "        q_network_theta = self.online_network.get_weights()\n",
    "        target_network_theta = self.target_network.get_weights()\n",
    "        counter = 0\n",
    "        for q_weight, target_weight in zip(q_network_theta,target_network_theta):\n",
    "            target_weight = target_weight * (1-TAU) + q_weight * TAU\n",
    "            target_network_theta[counter] = target_weight\n",
    "            counter += 1\n",
    "        self.target_network.set_weights(target_network_theta)\n",
    "\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        if len(self.memory) <= AGENT_MEMORY_LIMIT:\n",
    "            experience = (state, action, reward, next_state, done)\n",
    "            self.memory.append(experience)\n",
    "                  \n",
    "                  \n",
    "    def update_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon * EPSILON_DECAY, EPSILON_MIN)\n",
    "\n",
    "\n",
    "    def _reshape_state_for_net(self, state):\n",
    "        return np.reshape(state,(1, OBSERVATION_SPACE_DIMS))  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent():\n",
    "    env = gym.make('CartPole-v0')\n",
    "    env.seed(1)\n",
    "    trials = []\n",
    "    NUMBER_OF_TRIALS=10\n",
    "    MAX_TRAINING_EPISODES = 2000\n",
    "    MAX_STEPS_PER_EPISODE = 200\n",
    "\n",
    "    for trial_index in range(NUMBER_OF_TRIALS):\n",
    "        agent = DoubleDQNAgent()\n",
    "        trial_episode_scores = []\n",
    "\n",
    "        for episode_index in range(1, MAX_TRAINING_EPISODES+1):\n",
    "            state = env.reset()\n",
    "            episode_score = 0\n",
    "\n",
    "            for _ in range(MAX_STEPS_PER_EPISODE):\n",
    "                action = agent.act(state)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                episode_score += reward\n",
    "                agent.remember(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                if len(agent.memory) > MIN_MEMORY_FOR_EXPERIENCE_REPLAY:\n",
    "                    agent.experience_replay()\n",
    "                    agent.update_target_network()\n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            trial_episode_scores.append(episode_score)\n",
    "            agent.update_epsilon()\n",
    "            last_100_avg = np.mean(trial_episode_scores[-100:])\n",
    "            print (\"E %d scored %d, avg %.2f\" % (episode_index, episode_score, last_100_avg))\n",
    "            if len(trial_episode_scores) >= 100 and last_100_avg >= 195.0:\n",
    "                print (\"Trial %d solved in %d episodes!\" % (trial_index, (episode_index - 100)))\n",
    "                break\n",
    "        trials.append(np.array(trial_episode_scores))\n",
    "    return np.array(trials)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trials(trials):\n",
    "    _, axis = plt.subplots()    \n",
    "\n",
    "    for i, trial in enumerate(trials):\n",
    "        steps_till_solve = trial.shape[0]-100\n",
    "        # stop trials at 2000 steps\n",
    "        if steps_till_solve < 1900:\n",
    "            bar_color = 'b'\n",
    "            bar_label = steps_till_solve\n",
    "        else:\n",
    "            bar_color = 'r'\n",
    "            bar_label = 'Stopped at 2000'\n",
    "        plt.bar(np.arange(i,i+1), steps_till_solve, 0.5, color=bar_color, align='center', alpha=0.5)\n",
    "        axis.text(i-.25, steps_till_solve + 20, bar_label, color=bar_color)\n",
    "\n",
    "    plt.ylabel('Episodes Till Solve')\n",
    "    plt.xlabel('Trial')\n",
    "    trial_labels = [str(i+1) for i in range(len(trials))]\n",
    "    plt.xticks(np.arange(len(trials)), trial_labels)\n",
    "    # remove y axis labels and ticks\n",
    "    axis.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "    plt.tick_params(axis='both', left='off')\n",
    "\n",
    "    plt.title('Double DQN CartPole v-0 Trials')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_individual_trial(trial):\n",
    "    plt.plot(trial)\n",
    "    plt.ylabel('Steps in Episode')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.title('Double DQN CartPole v-0 Steps in Select Trial')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E 1 scored 23, avg 23.00\n",
      "E 2 scored 26, avg 24.50\n",
      "E 3 scored 13, avg 20.67\n",
      "E 4 scored 33, avg 23.75\n",
      "E 5 scored 10, avg 21.00\n",
      "E 6 scored 28, avg 22.17\n",
      "E 7 scored 13, avg 20.86\n",
      "E 8 scored 15, avg 20.12\n",
      "E 9 scored 10, avg 19.00\n",
      "E 10 scored 31, avg 20.20\n",
      "E 11 scored 12, avg 19.45\n",
      "E 12 scored 15, avg 19.08\n",
      "E 13 scored 9, avg 18.31\n",
      "E 14 scored 27, avg 18.93\n",
      "E 15 scored 15, avg 18.67\n",
      "E 16 scored 10, avg 18.12\n",
      "E 17 scored 20, avg 18.24\n",
      "E 18 scored 18, avg 18.22\n",
      "E 19 scored 18, avg 18.21\n",
      "E 20 scored 22, avg 18.40\n",
      "E 21 scored 11, avg 18.05\n",
      "E 22 scored 29, avg 18.55\n",
      "E 23 scored 8, avg 18.09\n",
      "E 24 scored 14, avg 17.92\n",
      "E 25 scored 23, avg 18.12\n",
      "E 26 scored 29, avg 18.54\n",
      "E 27 scored 10, avg 18.22\n",
      "E 28 scored 11, avg 17.96\n",
      "E 29 scored 10, avg 17.69\n",
      "E 30 scored 26, avg 17.97\n",
      "E 31 scored 9, avg 17.68\n",
      "E 32 scored 11, avg 17.47\n",
      "E 33 scored 9, avg 17.21\n",
      "E 34 scored 9, avg 16.97\n",
      "E 35 scored 9, avg 16.74\n",
      "E 36 scored 11, avg 16.58\n",
      "E 37 scored 11, avg 16.43\n",
      "E 38 scored 10, avg 16.26\n",
      "E 39 scored 12, avg 16.15\n",
      "E 40 scored 14, avg 16.10\n",
      "E 41 scored 13, avg 16.02\n",
      "E 42 scored 8, avg 15.83\n",
      "E 43 scored 11, avg 15.72\n",
      "E 44 scored 9, avg 15.57\n",
      "E 45 scored 12, avg 15.49\n",
      "E 46 scored 12, avg 15.41\n",
      "E 47 scored 12, avg 15.34\n",
      "E 48 scored 10, avg 15.23\n",
      "E 49 scored 11, avg 15.14\n",
      "E 50 scored 8, avg 15.00\n",
      "E 51 scored 9, avg 14.88\n",
      "E 52 scored 9, avg 14.77\n",
      "E 53 scored 11, avg 14.70\n",
      "E 54 scored 11, avg 14.63\n",
      "E 55 scored 66, avg 15.56\n",
      "E 56 scored 16, avg 15.57\n",
      "E 57 scored 30, avg 15.82\n",
      "E 58 scored 28, avg 16.03\n",
      "E 59 scored 22, avg 16.14\n",
      "E 60 scored 78, avg 17.17\n",
      "E 61 scored 79, avg 18.18\n",
      "E 62 scored 58, avg 18.82\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    trials = test_agent()\n",
    "    # print 'Saving', file_name\n",
    "    # np.save('double_dqn_cartpole_trials.npy', trials)\n",
    "    # trials = np.load('double_dqn_cartpole_trials.npy')\n",
    "    plot_trials(trials)\n",
    "    plot_individual_trial(trials[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
