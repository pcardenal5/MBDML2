{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD3 model\n",
    "\n",
    "This notebook contains the development process of the TD3 model. All classes have been declared on another file, TD3Aux.py, in order to clean up the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import TD3Aux\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[42]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 42\n",
    "# CREATING THE ENVIRONMENT\n",
    "env_name = \"Acrobot-v1\"\n",
    "env = gym.make(env_name)\n",
    "env.seed(seed)     # reproducible, general Policy gradient has high variance\n",
    "#state_low   = env.observation_space.low\n",
    "#state_high  = env.observation_space.high\n",
    "#action_low  = env.action_space.low \n",
    "#action_high = env.action_space.high\n",
    "#print(\"state_low   :\", state_low)\n",
    "#print(\"state_high  :\", state_high)\n",
    "#print(\"action_low  :\", action_low)\n",
    "#print(\"action_high :\", action_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALIZING THE Q-PARAMETERS\n",
    "hidden_size = 512\n",
    "max_episodes = 500  # Set total number of episodes to train agent on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -1.      ,  -1.      ,  -1.      ,  -1.      , -12.566371,\n",
       "       -28.274334], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.253524 0.       0.      ]], shape=(1, 3), dtype=float32)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\OneDrive\\Colegio_Uni\\Uni\\MBD\\Machine Learning 2\\Assignment1\\AssignmentRL\\Notebooks\\TD3.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive/Colegio_Uni/Uni/MBD/Machine%20Learning%202/Assignment1/AssignmentRL/Notebooks/TD3.ipynb#W5sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mget_action(state)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive/Colegio_Uni/Uni/MBD/Machine%20Learning%202/Assignment1/AssignmentRL/Notebooks/TD3.ipynb#W5sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# TAKING ACTION\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/OneDrive/Colegio_Uni/Uni/MBD/Machine%20Learning%202/Assignment1/AssignmentRL/Notebooks/TD3.ipynb#W5sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m next_state, reward, done, _ \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive/Colegio_Uni/Uni/MBD/Machine%20Learning%202/Assignment1/AssignmentRL/Notebooks/TD3.ipynb#W5sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m agent\u001b[39m.\u001b[39msavexp(state, action, reward, next_state, done)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive/Colegio_Uni/Uni/MBD/Machine%20Learning%202/Assignment1/AssignmentRL/Notebooks/TD3.ipynb#W5sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m agent\u001b[39m.\u001b[39mtrain_step()\n",
      "File \u001b[1;32mc:\\Users\\pcard\\miniconda3\\envs\\ml\\lib\\site-packages\\gym\\wrappers\\time_limit.py:18\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m     15\u001b[0m     \u001b[39massert\u001b[39;00m (\n\u001b[0;32m     16\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     ), \u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling reset()\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 18\u001b[0m     observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     19\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     20\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32mc:\\Users\\pcard\\miniconda3\\envs\\ml\\lib\\site-packages\\gym\\envs\\classic_control\\acrobot.py:112\u001b[0m, in \u001b[0;36mAcrobotEnv.step\u001b[1;34m(self, a)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, a):\n\u001b[0;32m    111\u001b[0m     s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\n\u001b[1;32m--> 112\u001b[0m     torque \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mAVAIL_TORQUE[a]\n\u001b[0;32m    114\u001b[0m     \u001b[39m# Add noise to the force action\u001b[39;00m\n\u001b[0;32m    115\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtorque_noise_max \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\pcard\\miniconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1065\u001b[0m, in \u001b[0;36m_EagerTensorBase.__index__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1064\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__index__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m-> 1065\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_numpy()\u001b[39m.\u001b[39;49m\u001b[39m__index__\u001b[39;49m()\n",
      "\u001b[1;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "# train\n",
    "agent = TD3Aux.Agent(\n",
    "    env, hidden_size\n",
    "#     memory_size, \n",
    "#     batch_size, \n",
    "#     epsilon_decay,\n",
    ")\n",
    "\n",
    "tf.random.set_seed(seed)\n",
    "# TRAINING LOOP\n",
    "#List to contain all the rewards of all the episodes given to the agent\n",
    "scores = []\n",
    "reward_records = []\n",
    "# EACH EPISODE    \n",
    "for episode in range(max_episodes):\n",
    "    ## Reset environment and get first new observation\n",
    "    state = agent.env.reset()\n",
    "    episode_reward = 0\n",
    "    done = False  # has the enviroment finished?\n",
    "    \n",
    "        \n",
    "    # EACH TIME STEP    \n",
    "    while not done:\n",
    "    # for step in range(max_steps):  # step index, maximum step is 200\n",
    "        action = agent.get_action(state)\n",
    "        \n",
    "        # TAKING ACTION\n",
    "        next_state, reward, done, _ = agent.env.step(action)\n",
    "        \n",
    "        agent.savexp(state, action, reward, next_state, done)\n",
    "        agent.train_step()\n",
    "        \n",
    "        # Our new state is state\n",
    "        state = next_state\n",
    "        \n",
    "        episode_reward += reward\n",
    "\n",
    "        # if episode ends\n",
    "        if done:\n",
    "            scores.append(episode_reward)\n",
    "            print(\"Episode \" + str(episode+1) + \": \" + str(episode_reward))\n",
    "            reward_records.append(episode_reward)\n",
    "            \n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Generate recent 50 interval average\n",
    "average_reward = []\n",
    "for idx in range(len(reward_records)):\n",
    "    avg_list = np.empty(shape=(1,), dtype=int)\n",
    "    if idx < 50:\n",
    "        avg_list = reward_records[:idx+1]\n",
    "    else:\n",
    "        avg_list = reward_records[idx-49:idx+1]\n",
    "    average_reward.append(np.average(avg_list))\n",
    "plt.plot(reward_records)  \n",
    "plt.plot(average_reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
