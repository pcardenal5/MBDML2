{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook de desarrollo del trabajo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizaje por Refuerzo\n",
    "\n",
    "El aprendizaje por refuerzo (RL) se basa en la idea de que un agente aprende a tomar decisiones interactuando con un entorno. De esta forma, el agente aprende a maximizar una recompensa basada en sus acciones, lo que le permite aprender a realizar tareas complejas a través de la exploración del entorno y la adaptación a los cambios en él.\n",
    "\n",
    "En el presente trabajo, se utilizará RL para entrenar a un agente que controle el movimiento del robot `Acrobot`, utilizando el paquete gym de OpenAI. \n",
    "\n",
    "+   El entorno Acrobot consiste en un robot de dos brazos que puede girar alrededor de su base. El robot se encuentra en un estado inicial colgando hacia abajo y debe alcanzar una posición objetivo que se define como el momento en que el extremo superior del segundo brazo del robot alcanza una altura específica.\n",
    "\n",
    "+   Para lograr este objetivo, el agente debe controlar los movimientos del robot mediante la aplicación de torque a sus articulaciones. El espacio de acción del agente se compone de tres acciones posibles: aplicar torque negativo a la primera articulación (-1), aplicar torque positivo a la primera articulación (1) y no aplicar torque (0). Estas acciones permiten al agente controlar el movimiento del robot y llevarlo hacia la posición objetivo."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivos establecidos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El `objetivo del trabajo` es entrenar un agente que sea capaz de controlar con éxito el movimiento del robot Acrobot y alcanzar la posición objetivo en el menor número de episodios posible. Esto se logra mediante el uso de técnicas de aprendizaje por refuerzo que permiten al agente aprender de manera eficiente a partir de sus interacciones con el entorno. Además se busca analizar y comparar la eficacia de los métodos seleccionados para resolver el problema y así identificar el método más eficiente para la tarea en cuestión."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métodos seleccionados"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La selección de los métodos de aprendizaje por refuerzo utilizados en este trabajo se basa en un análisis detallado del entorno Acrobot y en la adaptabilidad de los métodos de RL a este entorno en particular. Por lo tanto, se han elegido los métodos más adecuados para el problema que se desea resolver.\n",
    "\n",
    "+ `DQN` es una buena opción ya que es una extensión de Q-learning que utiliza una red neuronal profunda para aproximar el valor de Q. Se ha demostrado que funciona bien en entornos complejos y continuos como Acrobot, lo que lo convierte en una opción viable para nuestro problema.\n",
    "\n",
    "+ `Double DQN` es una mejora de DQN que ayuda a reducir la sobreestimación del valor de Q, lo que lo hace aún más efectivo en entornos complejos. Al igual que DQN, se ha demostrado que funciona bien en entornos continuos.\n",
    "\n",
    "+ `Reinforce` es un método de gradiente de políticas que funciona bien en entornos con acciones continuas y puede ser útil para explorar diferentes estrategias de acción en Acrobot. Esta es otra razón por la que se ha incluido en este trabajo.\n",
    "\n",
    "Por otro lado, `Q-learning` y `Double Q-learning` no se han utilizado debido a la naturaleza continua del espacio de observación y la complejidad del problema en sí mismo. `DDPG` tampoco se ha utilizado debido a la complejidad del problema y la dificultad de entrenar con éxito un agente utilizando este método. Por último, `TD3` se ha descartado por la misma razón que DDPG ya que es una mejora de este último."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerías utilizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entorno de trabajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"Acrobot-v1\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
